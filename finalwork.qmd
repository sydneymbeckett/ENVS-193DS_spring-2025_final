---
title: "Final"
author: "Sydney Beckett"
format: html
---

https://github.com/sydneymbeckett/ENVS-193DS_spring-2025_final.git 


```{r message= FALSE, warning=FALSE}
library(tidyverse) # general use
library(here) # file organization
library(gt) # creating summary tables
library(flextable) # creating summary tables
library(janitor) # cleaning data frames
library(lubridate) 
sst <- read_csv("data/SST_update2023.csv")
```

## Problem 1.

**a)**

In part 1 they likely used a Pearsons correlation test because they were comparing two continuous variables to test for presence of correlation.
In part 2 they likely used a one-way ANOVA because they were comparing one continuous variable "between sources", which is more than 2 groups.

**b)**
Other information that would be helpful would be the sample size which would help reproducibility, statistical power(aka if the sample size is small then the results might not be as powerful) and reliability if there were outliers(small sample size would highly impacted by one outlier). And given they used a one-way ANOVA, another piece of information that would be helpful would be the F-value which represents the ratio of between group variance to within group variance. Essentially a larger F-value means the group means differ more relative to the variation within groups. 

**c)**
Part 1: 
The test resulted in a rejection of the null hypothesis which stated that there is no correlation between distance from headwater (km) and annual total nitrogen load (kg year-1). The relationship was evaluated using Pearsons correlation test(r=correlation coefficient, p=0.03, α = significance level, n=sample size).

Part 2: 

We rejected the null hypothesis that there is no difference in average nitrogen load (kg year-1) between sources (urban land, atmospheric deposition, fertilizer, waste water treatment, and grasslands). This was determined using a one-way ANOVA(F= F-statistic, p=0.02, α=significance level, df=degrees of freedom, SS=sum of squares, n=sample size).

## Problem 2.

**a)**
```{r}
sst_clean <- sst |> #use the sst data frame
  clean_names() |> #clean up the column names
  mutate(date = ymd(date)) |>  #making sure the date is read as a date
  mutate(
  year = (year(date)), #making the year a date
    month = month(date, label = TRUE, abbr = TRUE)) |> #making the month category and using abbreviations 
  filter(year >= 2018) |>  # only include recent 6 years) 
  group_by(year, month) |> #grouping by year and month
    summarize(
    mean_monthly_sst = round(mean(temp, na.rm = TRUE), 1), #calculate the mean monthly sst and round out to one decimal place
    .groups = "drop" # ungroup after summarize
  ) |> 
    mutate(year = factor(year), #converting to a factor after filtering
      month = factor(month, levels = month.abb, ordered = TRUE)) #ensure correct month ordering

sst_clean |> slice_sample(n = 5) #show 5 first rows

str(sst_clean) #show structure
  

```


